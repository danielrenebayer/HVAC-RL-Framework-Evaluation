{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "threaded-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "glp = os.path.abspath(\"../code\")\n",
    "if not glp in sys.path: sys.path.append( glp )\n",
    "\n",
    "from global_paths import global_paths\n",
    "\n",
    "if not global_paths[\"COBS\"] in sys.path: sys.path.append( global_paths[\"COBS\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "genetic-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cobs\n",
    "import torch\n",
    "import datetime\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "broadband-supplement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BuildingOccupancy import Person, Meeting, WeeklyMeeting, OneTimeMeeting, BuildingOccupancy\n",
    "import DefaultBuildings\n",
    "from Agents import agent_constructor\n",
    "from CentralController import ddpg_episode_mc\n",
    "import RLCritics\n",
    "import StateUtilities as SU\n",
    "from Options import get_argparser\n",
    "from ReplayBuffer import ReplayBufferStd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-vulnerability",
   "metadata": {},
   "source": [
    "Documentation: https://cobs-platform.github.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "satellite-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_argparser().parse_args([\n",
    "        \"--algorithm\", \"ddqn\",\n",
    "        \"--idf_file\",  \"\",\n",
    "        \"--epw_file\",  os.path.abspath(\"../../COBS/cobs/data/weathers/8.epw\"),\n",
    "        \"--checkpoint_dir\", \"/tmp\",\n",
    "        \"--shared_network_per_agent_class\"\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "worst-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#args = argparse.Namespace(\n",
    "#    algorithm = \"ddpg\",\n",
    "#    idf_file  = \"\",\n",
    "#    epw_file  = os.path.abspath(\"../../COBS/cobs/data/weathers/8.epw\"),\n",
    "#    checkpoint_dir = \"/tmp\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-guitar",
   "metadata": {},
   "source": [
    "<h3>Define the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "center-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "cobs.Model.set_energyplus_folder(global_paths[\"eplus\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "excess-ordinance",
   "metadata": {},
   "outputs": [],
   "source": [
    "building = DefaultBuildings.Building_5ZoneAirCooled_SingleSetpoint(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "essential-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_occ = BuildingOccupancy()\n",
    "building_occ.set_room_settings(building.room_names[:-1], {building.room_names[-1]: 40}, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "neural-bunny",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "building_occ.generate_random_occupants(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dental-surfing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "building_occ.generate_random_meetings(10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-peace",
   "metadata": {},
   "source": [
    "<h3>Define the agents (and the controlled devices)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "mineral-porcelain",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #\n",
    "    # Define the agents\n",
    "    agents = []\n",
    "    idx    = 0\n",
    "    # HINT: a device can be a zone, too\n",
    "    for agent_name, (controlled_device, controlled_device_type) in building.agent_device_pairing.items():\n",
    "        new_agent = agent_constructor( controlled_device_type )\n",
    "        new_agent.initialize(\n",
    "                         name = agent_name,\n",
    "                         args = args,\n",
    "                         controlled_element = controlled_device,\n",
    "                         global_state_keys  = building.global_state_variables)\n",
    "        agents.append(new_agent)\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "freelance-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building.agent_device_pairing\n",
    "#building.global_state_variables\n",
    "#agents[0].trafo_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-holder",
   "metadata": {},
   "source": [
    "<h3>Set model parameters</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "according-manor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.set_runperiod(365, 2020, 1, 1)\n",
    "building.model.set_runperiod(30, 2020, 1, 1)\n",
    "##building.model.set_runperiod(12, 2020, 7, 1)\n",
    "#model.set_runperiod(30, 1915, 12, 12, specify_year=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "furnished-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "building.model.set_timestep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-sellers",
   "metadata": {},
   "source": [
    "<h4>Prepare the simulation</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "independent-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_output_lists = []\n",
    "ts_diff_in_min = 60\n",
    "evaluation_epoch = False\n",
    "hyper_params = args\n",
    "LAMBDA_REWARD_ENERGY = hyper_params.lambda_rwd_energy\n",
    "LAMBDA_REWARD_MANU_STP_CHANGES = hyper_params.lambda_rwd_mstpc\n",
    "TAU_TARGET_NETWORKS  = hyper_params.tau\n",
    "DISCOUNT_FACTOR = hyper_params.discount_factor\n",
    "BATCH_SIZE      = hyper_params.batch_size\n",
    "RPB_BUFFER_SIZE = hyper_params.rpb_buffer_size\n",
    "LEARNING_RATE   = hyper_params.lr\n",
    "\n",
    "episode_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "removed-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.05\n",
    "for agent in agents:\n",
    "    agent.epsilon = epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-fields",
   "metadata": {},
   "source": [
    "<h3>Run the simulation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-intake",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #\n",
    "    # Define the replay ReplayBuffer\n",
    "    rpb = ReplayBufferStd(size=RPB_BUFFER_SIZE, number_agents=len(agents))\n",
    "    #\n",
    "    # Define the loss\n",
    "    loss = torch.nn.MSELoss()\n",
    "    #\n",
    "    # prepare the simulation\n",
    "    state = building.model_reset()\n",
    "    SU.fix_year_confussion(state)\n",
    "    norm_state_ten = SU.unnormalized_state_to_tensor(state, building)\n",
    "    #\n",
    "    current_occupancy = building_occ.draw_sample( state[\"time\"] )\n",
    "    timestep   = 0\n",
    "    last_state = None\n",
    "    # start the simulation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "biological-columbus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep.   0, ts.   200: 2017-01-09 09:00:00, no eval ep.,    rand. p. add.\n",
      "ep.   0, ts.   400: 2017-01-17 17:00:00, no eval ep.,    rand. p. add.\n",
      "ep.   0, ts.   600: 2017-01-26 01:00:00, no eval ep.,    rand. p. add.\n"
     ]
    }
   ],
   "source": [
    "    while not building.model_is_terminate():\n",
    "        actions = list()\n",
    "\n",
    "        currdate = state['time']\n",
    "        #\n",
    "        # request occupancy for the next state\n",
    "        nextdate = state['time'] + datetime.timedelta(minutes=ts_diff_in_min)\n",
    "        next_occupancy = building_occ.draw_sample(nextdate)\n",
    "        #\n",
    "        # propagate occupancy values to COBS / EnergyPlus\n",
    "        for zonename, occd in next_occupancy.items():\n",
    "            actions.append({\"priority\":        0,\n",
    "                            \"component_type\": \"Schedule:Constant\",\n",
    "                            \"control_type\":   \"Schedule Value\",\n",
    "                            \"actuator_key\":  f\"OCC-SCHEDULE-{zonename}\",\n",
    "                            \"value\":           next_occupancy[zonename][\"relative number occupants\"],\n",
    "                            \"start_time\":      state['timestep'] + 1})\n",
    "\n",
    "        #\n",
    "        # request new actions from all agents\n",
    "        agent_actions_dict = {}\n",
    "        agent_actions_list = []\n",
    "        add_random_process = True\n",
    "        if evaluation_epoch and not add_epsilon_greedy_in_eval_epoch:\n",
    "            add_random_process = False\n",
    "        if agents[0].shared_network_per_agent_class:\n",
    "            new_actions = agents[0].next_action(norm_state_ten, add_random_process)\n",
    "            agent_actions_list = new_actions\n",
    "            # decode the actions for every agent using the individual agent objects\n",
    "            for idx, agent in enumerate(agents):\n",
    "                agent_actions_dict[agent.name] = agent.output_action_to_action_dict(new_actions[idx])\n",
    "        else:\n",
    "            for agent in agents:\n",
    "                new_action = agent.next_action(norm_state_ten, add_random_process)\n",
    "                agent_actions_list.append( new_action )\n",
    "                agent_actions_dict[agent.name] = agent.output_action_to_action_dict(new_action)\n",
    "            # no backtransformation of variables needed, this is done in agents definition already\n",
    "\n",
    "        #\n",
    "        # send agent actions to the building object and obtaion the actions for COBS/eplus\n",
    "        actions.extend( building.obtain_cobs_actions( agent_actions_dict, state[\"timestep\"]+1 ) )\n",
    "\n",
    "        #\n",
    "        # send actions to EnergyPlus and obtian the new state\n",
    "        norm_state_ten_last = norm_state_ten\n",
    "        last_state = state\n",
    "        timestep  += 1\n",
    "        state      = building.model_step(actions)\n",
    "        current_occupancy = next_occupancy\n",
    "        SU.fix_year_confussion(state)\n",
    "\n",
    "        current_energy_Wh = state[\"energy\"] / 360\n",
    "\n",
    "        #\n",
    "        # modify state\n",
    "        norm_state_ten = SU.unnormalized_state_to_tensor(state, building)\n",
    "\n",
    "        #\n",
    "        # send current temp/humidity values for all rooms\n",
    "        # obtain number of manual setpoint changes\n",
    "        _, n_manual_stp_changes = building_occ.manual_setpoint_changes(state['time'], state[\"temperature\"], None)\n",
    "\n",
    "        #\n",
    "        # reward computation\n",
    "        if hyper_params is None or hyper_params.reward_function == \"sum_energy_mstpc\":\n",
    "            reward = -( LAMBDA_REWARD_ENERGY * current_energy_Wh + LAMBDA_REWARD_MANU_STP_CHANGES * n_manual_stp_changes )\n",
    "        elif hyper_params.reward_function == \"rulebased_roomtemp\":\n",
    "            reward = - reward_fn_rulebased_roomtemp(state, building)\n",
    "        #elif hyper_params.reward_function == \"rulebased_agent_output\":\n",
    "        else:\n",
    "            reward = - reward_fn_rulebased_agent_output(state, agent_actions_dict)\n",
    "        if not hyper_params is None and hyper_params.log_reward:\n",
    "            reward = - np.log(-reward + 1)\n",
    "\n",
    "        #\n",
    "        # save (last_state, actions, reward, state) to replay buffer\n",
    "        rpb.add_transition(norm_state_ten_last, agent_actions_list, reward, norm_state_ten)\n",
    "\n",
    "        #\n",
    "        # sample minibatch\n",
    "        b_state1, b_action, b_reward, b_state2 = rpb.sample_minibatch(BATCH_SIZE, False)\n",
    "        b_action = torch.tensor(b_action)\n",
    "\n",
    "        #\n",
    "        # loop over all [agent, critic]-pairs\n",
    "        if agents[0].shared_network_per_agent_class:\n",
    "            #\n",
    "            # compute y (i.e. the TD-target)\n",
    "            #  Hint: s_{i+1} <- state2; s_i <- state1\n",
    "            agents[0].model_actor.zero_grad()\n",
    "            #b_reward = b_reward.detach().expand(-1, len(agents) ).flatten()[:, np.newaxis]\n",
    "            b_reward = b_reward.detach().repeat(len(agents), 1)\n",
    "            y = b_reward + DISCOUNT_FACTOR * agents[0].step_tensor(b_state2, use_actor = False).detach().max(dim=1).values[:, np.newaxis]\n",
    "            # compute Q for state1\n",
    "            q = agents[0].step_tensor(b_state1, use_actor = True).gather(1, b_action.flatten()[:, np.newaxis])\n",
    "            # update agent by minimizing the loss L\n",
    "            L = loss(q, y)\n",
    "            L.backward()\n",
    "            agents[0].optimizer_step()\n",
    "        else:\n",
    "          for agent_id, agent in enumerate(agents):\n",
    "            #\n",
    "            # compute y (i.e. the TD-target)\n",
    "            #  Hint: s_{i+1} <- state2; s_i <- state1\n",
    "            agent.model_actor.zero_grad()\n",
    "            y = b_reward.detach() + DISCOUNT_FACTOR * agent.step_tensor(b_state2, use_actor = False).detach().max(dim=1).values[:, np.newaxis]\n",
    "            # compute Q for state1\n",
    "            q = agent.step_tensor(b_state1, use_actor = True).gather(1, b_action[:, agent_id][:, np.newaxis])\n",
    "            # update agent by minimizing the loss L\n",
    "            L = loss(q, y)\n",
    "            L.backward()\n",
    "            agent.optimizer_step()\n",
    "\n",
    "        if timestep % 20 == 0:\n",
    "            eval_ep_str     = \"  \" if evaluation_epoch   else \"no\"\n",
    "            rand_pr_add_str = \"  \" if add_random_process else \"no\"\n",
    "            if timestep % 200 == 0:\n",
    "                print(f\"ep. {episode_number:3}, ts. {timestep:5}: {state['time']}, {eval_ep_str} eval ep., {rand_pr_add_str} rand. p. add.\")\n",
    "            #else:\n",
    "            #    print(f\"ep. {episode_number:3}, ts. {timestep:5}: {state['time']}, {eval_ep_str} eval ep., {rand_pr_add_str} rand. p. add.\", end=\"\\r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #\n",
    "    # update target network for actors\n",
    "    status_output_dict[\"target_network_update\"] = False\n",
    "    if not evaluation_epoch and episode_number % 3 == 0:\n",
    "        for agent in agents:\n",
    "            agent.copy_weights_to_target()\n",
    "        status_output_dict[\"target_network_update\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-siemens",
   "metadata": {},
   "source": [
    "<h3>Evaluate the simulation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_list = output_lists[\"timestamp_list\"]\n",
    "episode_list   = output_lists[\"episode_list\"]\n",
    "df_indexes     = [episode_list, timestamp_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame(output_lists[\"loss_list\"], index = df_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "room_temp = pd.DataFrame(output_lists[\"room_temp_list\"], index = df_indexes)\n",
    "humidity  = pd.DataFrame(output_lists[\"humidity_list\"],  index = df_indexes)\n",
    "co2_ppm   = pd.DataFrame(output_lists[\"co2_ppm_list\"],   index = df_indexes)\n",
    "energy    = pd.DataFrame(output_lists[\"energy_list\"],    index = df_indexes)\n",
    "outd_temp = pd.DataFrame(output_lists[\"outd_temp_list\"], index = df_indexes)\n",
    "outd_humi = pd.DataFrame(output_lists[\"outd_humi_list\"], index = df_indexes)\n",
    "outd_solar_radi = pd.DataFrame(output_lists[\"outd_solar_radi_list\"], index = df_indexes)\n",
    "outd_wind_speed = pd.DataFrame(output_lists[\"outd_wspeed_list\"],     index = df_indexes)\n",
    "outd_wind_dir   = pd.DataFrame(output_lists[\"outd_wdir_list\"],       index = df_indexes)\n",
    "n_manual_stp_ch = pd.DataFrame(output_lists[\"n_manual_stp_ch_list\"], index = df_indexes)\n",
    "rewards   = pd.DataFrame(output_lists[\"rewards_list\"],   index = df_indexes)\n",
    "\n",
    "vav_pos   = pd.DataFrame(output_lists[\"vav_pos_list\"],   index = df_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-courage",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy_list_abs = [{k: v[\"absolute number occupants\"] for k,v in d.items() } for d in output_lists[\"occupancy_list\"]]\n",
    "occupancy_list_rel = [{k: v[\"relative number occupants\"] for k,v in d.items() } for d in output_lists[\"occupancy_list\"]]\n",
    "occupancy_list_meanT = [{k: v[\"mean comfort temp\"] for k,v in d.items() } for d in output_lists[\"occupancy_list\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy_absolute = pd.DataFrame(occupancy_list_abs, index = df_indexes)\n",
    "occupancy_relative = pd.DataFrame(occupancy_list_rel, index = df_indexes)\n",
    "occupancy_meantemp = pd.DataFrame(occupancy_list_meanT, index = df_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_episode in range(n_episode_runs):\n",
    "    energy_sum_in_kwh = round( energy.loc[n_episode].sum()[0] / 360000, 3)\n",
    "    print(f\"Total energy used during episode {n_episode:3}: {energy_sum_in_kwh:11} kWh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df.plot(title=\"Loss\", figsize=(14,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_manual_stp_ch.plot(title=\"Number of manual setpoint changes\", figsize=(14,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards.plot(title=\"Rewards\", figsize=(14,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-discussion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy_absolute.plot(title=\"Indoor occupancy\", ylabel=\"Number of people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupancy_meantemp.plot(title=\"comfort temperature\", ylabel=\"°C\", ylim=[17,25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "room_temp.plot(title=\"Indoor temperature\", ylabel=\"°C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "humidity.plot(title=\"Indoor humidity\", ylabel=\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_ppm.plot(title=\"Indoor CO2 concentration\", ylabel=\"ppm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "(energy/3600000).plot(ylabel=\"kWh\", title=\"Energieverbrauch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "outd_temp.plot(ylabel=\"°C\", title=\"Outdoor temerature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "outd_humi.plot(ylabel=\"%\", title=\"Outdoor humidity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-utility",
   "metadata": {},
   "outputs": [],
   "source": [
    "outd_wind_speed.plot(title=\"Wind speed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "outd_wind_dir.plot(title=\"Wind direction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "outd_solar_radi.plot(title=\"Solar radiation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# controllable elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-print",
   "metadata": {},
   "outputs": [],
   "source": [
    "vav_pos.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-account",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
